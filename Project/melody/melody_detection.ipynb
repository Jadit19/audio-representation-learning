{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set\"\n",
    "\n",
    "FRAME_SIZE = 50\n",
    "BPF_LOW = 50\n",
    "BPF_HIGH = 1000\n",
    "\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 256\n",
    "\n",
    "MARGIN_I, MARGIN_V = 2, 10\n",
    "POWER = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_files = [dataset_path + \"/\" + f.name for f in list(os.scandir(dataset_path)) if f.name.endswith('.wav')]\n",
    "data_files = [f.replace(\".wav\", \"REF.txt\") for f in wav_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(data_file):\n",
    "  data = []\n",
    "  with open(data_file, 'r') as f:\n",
    "    raw_data = f.readlines()\n",
    "    for line in raw_data:\n",
    "      data.append([float(x) for x in line.strip().split()])\n",
    "  \n",
    "  annotations = []\n",
    "  for i in range(1, len(data)):\n",
    "    temp = int(data[i][0] * 1000) - int(data[i-1][0] * 1000)\n",
    "    while (temp > 0):\n",
    "      temp -= 1\n",
    "      annotations.append(data[i-1][1])\n",
    "\n",
    "  frames = []\n",
    "  for i in range(0, len(annotations)-FRAME_SIZE, FRAME_SIZE):\n",
    "    frame = [i/1000, (i+FRAME_SIZE)/1000, 0]\n",
    "    pitch = 0\n",
    "    for j in range(i, i+FRAME_SIZE):\n",
    "      pitch += annotations[j]\n",
    "    pitch /= FRAME_SIZE\n",
    "    frame[2] = pitch\n",
    "    frames.append(frame)\n",
    "  \n",
    "  segments = []\n",
    "  for frame in frames:\n",
    "    if frame[2] != 0:\n",
    "      segments.append(frame)\n",
    "  segments = np.array(segments)\n",
    "  \n",
    "  # plt.figure(figsize=(20, 5))\n",
    "  # plt.scatter(segments[:, 0], segments[:, 2])\n",
    "  # plt.title(\"Annotations\")\n",
    "  # plt.xlabel(\"Time (s)\")\n",
    "  # plt.ylabel(\"Pitch (Hz)\")\n",
    "  # plt.grid()\n",
    "  # plt.show()\n",
    "  \n",
    "  return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(y, sr):\n",
    "  nyquist = 0.5 * sr\n",
    "  low = BPF_LOW / nyquist\n",
    "  high = BPF_HIGH / nyquist\n",
    "  b, a = butter(5, [low, high], btype='band')\n",
    "  data = lfilter(b, a, y)\n",
    "  return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(wav_file, data_file):\n",
    "  y, sr = librosa.load(wav_file)\n",
    "  y_filtered = bandpass_filter(y, sr)\n",
    "  \n",
    "  segments = get_segments(data_file)\n",
    "  for segment in segments:\n",
    "    start, end, pitch = segment\n",
    "    y_segment = y_filtered[int(start*sr) : int(end*sr)]    \n",
    "    s_full, phase = librosa.magphase(librosa.stft(y_segment, n_fft=N_FFT, hop_length=HOP_LENGTH))\n",
    "    s_filter = librosa.decompose.nn_filter(s_full, aggregate=np.median, metric='cosine')\n",
    "    s_filter = np.minimum(s_full, s_filter)\n",
    "    mask_v = librosa.util.softmask(s_full-s_filter, MARGIN_V * s_filter, power=POWER)\n",
    "    s_foreground = mask_v * s_full\n",
    "    spec_db_foregound = librosa.amplitude_to_db(s_foreground, ref=np.max)\n",
    "    \n",
    "    # plt.figure(figsize=(20, 5))\n",
    "    # librosa.display.specshow(spec_db_foregound, sr=sr, x_axis='time', y_axis='log')\n",
    "    # plt.colorbar()\n",
    "    # plt.title(\"Spectrogram\")\n",
    "    # plt.xlabel(\"Time (s)\")\n",
    "    # plt.ylabel(\"Frequency (Hz)\")\n",
    "    # plt.show()\n",
    "    \n",
    "    x_data.append(spec_db_foregound.flatten())\n",
    "    y_data.append(pitch)\n",
    "\n",
    "  print(wav_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/jazz2.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/jazz1.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/midi4.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/daisy3.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/pop4.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/midi1.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/daisy2.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/opera_male5.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/jazz3.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/jazz4.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/opera_fem4.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/daisy1.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/daisy4.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/pop3.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/pop2.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/pop1.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/opera_fem2.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/midi2.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/opera_male3.wav\n",
      "/home/adit/Downloads/EE798P/Datasets/Melody Estimation/adc2004_full_set/midi3.wav\n"
     ]
    }
   ],
   "source": [
    "max_threads = 8\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "  futures = [executor.submit(make_data, wav_file, data_file) for wav_file, data_file in zip(wav_files, data_files)]\n",
    "  concurrent.futures.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "x_train, x_test, y_train, y_test = np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 18553.55363229\n",
      "Iteration 2, loss = 14046.81101381\n",
      "Iteration 3, loss = 13911.93675717\n",
      "Iteration 4, loss = 13807.75219208\n",
      "Iteration 5, loss = 14626.05340489\n",
      "Iteration 6, loss = 13838.14359192\n",
      "Iteration 7, loss = 13402.54642544\n",
      "Iteration 8, loss = 13024.61948730\n",
      "Iteration 9, loss = 13286.29726332\n",
      "Iteration 10, loss = 12550.14069251\n",
      "Iteration 11, loss = 12218.16999043\n",
      "Iteration 12, loss = 12552.31187962\n",
      "Iteration 13, loss = 13110.23909429\n",
      "Iteration 14, loss = 11984.27940785\n",
      "Iteration 15, loss = 12296.51500547\n",
      "Iteration 16, loss = 11891.90155636\n",
      "Iteration 17, loss = 11553.74786334\n",
      "Iteration 18, loss = 11148.56404451\n",
      "Iteration 19, loss = 10892.04925496\n",
      "Iteration 20, loss = 11312.78117622\n",
      "Iteration 21, loss = 11187.00284014\n",
      "Iteration 22, loss = 11223.79304632\n",
      "Iteration 23, loss = 10271.21327164\n",
      "Iteration 24, loss = 10831.01068701\n",
      "Iteration 25, loss = 10301.90648861\n",
      "Iteration 26, loss = 10404.04649447\n",
      "Iteration 27, loss = 10417.07748462\n",
      "Iteration 28, loss = 10421.80294340\n",
      "Iteration 29, loss = 11106.06629164\n",
      "Iteration 30, loss = 10090.92160147\n",
      "Iteration 31, loss = 10923.08351515\n",
      "Iteration 32, loss = 10166.17033603\n",
      "Iteration 33, loss = 9928.79054299\n",
      "Iteration 34, loss = 9542.41153592\n",
      "Iteration 35, loss = 11046.48251993\n",
      "Iteration 36, loss = 9929.46185651\n",
      "Iteration 37, loss = 9691.46773700\n",
      "Iteration 38, loss = 9551.16308506\n",
      "Iteration 39, loss = 10026.00670999\n",
      "Iteration 40, loss = 9411.67668003\n",
      "Iteration 41, loss = 10353.59735368\n",
      "Iteration 42, loss = 9175.47164115\n",
      "Iteration 43, loss = 9512.50343884\n",
      "Iteration 44, loss = 10991.53371322\n",
      "Iteration 45, loss = 9558.49315869\n",
      "Iteration 46, loss = 9256.19527624\n",
      "Iteration 47, loss = 9225.90393374\n",
      "Iteration 48, loss = 8911.47728247\n",
      "Iteration 49, loss = 9482.28885835\n",
      "Iteration 50, loss = 9037.96689984\n",
      "Iteration 51, loss = 10388.21005030\n",
      "Iteration 52, loss = 9961.50988375\n",
      "Iteration 53, loss = 9080.38253639\n",
      "Iteration 54, loss = 9999.05714767\n",
      "Iteration 55, loss = 8600.59825614\n",
      "Iteration 56, loss = 9324.33747737\n",
      "Iteration 57, loss = 8338.07898480\n",
      "Iteration 58, loss = 8805.82397228\n",
      "Iteration 59, loss = 8612.91874472\n",
      "Iteration 60, loss = 8170.01413088\n",
      "Iteration 61, loss = 8625.92621312\n",
      "Iteration 62, loss = 8517.49047751\n",
      "Iteration 63, loss = 8441.39980039\n",
      "Iteration 64, loss = 9313.25183443\n",
      "Iteration 65, loss = 9687.71396204\n",
      "Iteration 66, loss = 9274.84400778\n",
      "Iteration 67, loss = 8581.37889041\n",
      "Iteration 68, loss = 9776.35782422\n",
      "Iteration 69, loss = 8838.65153975\n",
      "Iteration 70, loss = 8597.33401941\n",
      "Iteration 71, loss = 8217.16569336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=(100, 100, 100), max_iter=100, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=(100, 100, 100), max_iter=100, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(100, 100, 100), max_iter=100, verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(100, 100, 100), max_iter=100, verbose=True)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 96.97\n"
     ]
    }
   ],
   "source": [
    "# Evalutaing MAE\n",
    "y_pred = model.predict(x_test)\n",
    "mae = np.mean(np.abs(y_pred - y_test))\n",
    "print(\"MAE: %.2f\" % mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
